# Synthetic QA Generation Configuration

# Global settings
global:
  max_chunk_size: 4000
  min_chunk_size: 1000
  overlap_size: 200

# Processing settings for specific file types
processing:
  faq:
    enabled: true
    generate_rephrased_questions: true
    generate_related_questions: true
    max_rephrased_questions: 7
    max_related_questions: 3
    context_size: 3
    batch_size: 10

  raft:
    num_distractors: 3
    p_golden_include: 0.8



# Iterations by document type
iterations:
  regular_document: 1
  faq_document: 1

# Question generation style variations
question_styles:
  writing_styles:
    - name: 'Default'
      goal: 'Establish a clean, well-formed baseline; represent clear, unambiguous queries.'
      description: 'Generate a clear, straightforward question using complete sentences. Avoid excessive formality or informality. Focus on accurately representing the core content of the original question/chunk. **Crucially: If the original answer/chunk was very short (like "Sim.", "Não."), ensure this question naturally elicits the expanded, polite answer generated for the Default pair.** The question should sound like a typical, clear query a student might ask an assistant.'
      iterations: 1
    
    - name: 'Formal'
      goal: 'Ensure the Final LLM handles polite, grammatically correct, as a student would write it.'
      description: 'Generate a polite question using grammatically correct, complete sentences and a slightly formal tone. Represent a student interacting respectfully and formally, but avoid overly complex, archaic language or being prolix.'
      iterations: 2

    - name: 'Casual'
      goal: 'Simulate common, less formal but still complete-sentence interactions; improve robustness to everyday language.'
      description: 'Generate a question using natural, conversational, simpler Portuguese, as a Brazilian student might ask directly in a chat. Use complete sentences but avoid complex phrasing. **Crucial: Maintain clear grammar and politeness; do NOT use slang, abbreviations, or excessive informality.** Example: "Como faço pra trancar?" instead of "Qual o procedimento para trancamento?".'
      iterations: 2

    - name: 'Naturalistic/Imperfect'
      goal: 'Capture realistic, quickly typed/spoken inputs that use full sentences but contain common grammatical errors, unconventional word order, or minor typos, without losing the core meaning. Improve robustness to non-standard but understandable inputs.'
      description: 'Generate a question as a student might type quickly into a chat. Use full sentences but **intentionally introduce common, realistic grammatical errors (e.g., verb conjugation, concordance), slightly unconventional word order, or minor typos.** The core meaning and intent **must remain clear and easily understandable**, but the sentence structure should not be perfectly polished. Avoid slang or keyword-only fragments. Examples: "eu queria saber como faz pra trancar a materia", "se eu reprovar no verao conta pro desligamento?", "qual o prazo minimo engenharia da computacao pra formar?".'
      iterations: 2


    # - name: 'Direct'
    #   goal: 'Ensure the Final LLM understands and responds correctly to minimal, keyword-based, or fragmented queries, similar to search engine inputs.'
    #   description: 'Generate a short, direct query, potentially using **keywords or essential phrases rather than full grammatical sentences.** Mimic a user typing a search query or asking a very brief, "lazy" but understandable question. Often lacks punctuation like question marks. Examples: "trancamento justificado regras", "prazo máximo Eng Comp", "Enade obrigatório?".'

  question_types:
    - name: 'Informational'
      description: 'Seek factual information. The "What".'
    - name: 'Procedural'
      description: 'Ask about processes, steps, or how to do things. The "How"'
    - name: 'Conceptual'
      description: 'Seek explanation or understanding of concepts or policies. The "Why"'

# API providers configuration
providers:
  faq_extraction:
    provider: genai
    model: gemini-2.0-flash
    temperature: 0.4
    max_tokens: 65000

  default_question:
    provider: genai
    model: gemini-2.5-flash-preview-05-20
    rate_limit_rpm: 4
    temperature: 0.4
    max_tokens: 16384
    thinking_budget: 4096

  default_answer:
    provider: genai
    model: gemini-2.5-flash-preview-05-20
    rate_limit_rpm: 4
    temperature: 0.4
    max_tokens: 16384
    thinking_budget: 4096

  styled_question:
    provider: genai
    model: gemini-2.5-flash-preview-05-20
    rate_limit_rpm: 4
    temperature: 0.4
    max_tokens: 8192

  cot_answer:
    provider: genai
    model: gemini-2.5-flash-preview-05-20
    rate_limit_rpm: 4
    temperature: 0.4
    max_tokens: 8192
    thinking_budget: 512

  related_question:
    provider: genai # openai
    model: gemini-2.0-flash
    rate_limit_rpm: 14
    temperature: 0.4
    max_tokens: 8192

  rephrased_question:
    provider: genai # openai
    model: gemini-2.0-flash
    rate_limit_rpm: 14
    temperature: 0.4
    max_tokens: 8192
  
  answer:
    provider: genai
    model: gemini-2.0-flash
    rate_limit_rpm: 14
    temperature: 0.5
    max_tokens: 8192
  
  text_extraction:
    provider: genai
    model: gemini-2.5-flash-preview-05-20
    rate_limit_rpm: 4
    temperature: 0.3
    max_tokens: 65000
    thinking_budget: 4096

  text_chunking:
    provider: genai
    model: gemini-2.5-flash-preview-05-20
    rate_limit_rpm: 4
    temperature: 0.2
    max_tokens: 65000
    thinking_budget: 4096

# File processing settings
file_processing:
  include_extensions:
    - .html
    - .htm
    - .pdf
    - .txt
    - .md
  
  exclude_patterns:
    - images/
    - assets/
    - .git/
    - styles/
    - js/

# Output settings
output:
  save_debug: true
  format: json
  include_source: true
